Fundamental Concepts
What is Machine Learning?
Machine Learning (ML) is a subset of artificial intelligence that enables systems to learn patterns from data and make predictions or decisions without being explicitly programmed.

Difference between Supervised, Unsupervised, and Reinforcement Learning.

Supervised Learning: Uses labeled data to predict outcomes (e.g., Regression, Classification).
Unsupervised Learning: Identifies patterns in unlabeled data (e.g., Clustering, Dimensionality Reduction).
Reinforcement Learning: Learns through trial and error by receiving rewards or penalties (e.g., Q-Learning).
Difference between AI, Machine Learning, and Deep Learning.

AI: Broad field for creating intelligent systems.
ML: Subset of AI focused on learning from data.
Deep Learning: Subset of ML using neural networks with multiple layers.
Explain Overfitting and Underfitting. How do you prevent them?

Overfitting: Model fits training data too well but performs poorly on new data.
Prevention: Use regularization (L1, L2), pruning, or cross-validation.
Underfitting: Model is too simple and fails to capture the data's complexity.
Prevention: Increase model complexity or train longer.
Difference between Parametric and Non-Parametric Models.

Parametric Models: Fixed number of parameters (e.g., Linear Regression).
Non-Parametric Models: Number of parameters grows with the data (e.g., KNN).
Regression Questions
What is Linear Regression? How does it work?
Linear Regression models the relationship between dependent and independent variables using a linear equation:
ğ‘¦
=
ğ›½
0
+
ğ›½
1
ğ‘¥
+
ğœ–
y=Î² 
0
â€‹
 +Î² 
1
â€‹
 x+Ïµ
where 
ğ›½
0
Î² 
0
â€‹
  is the intercept, 
ğ›½
1
Î² 
1
â€‹
  is the slope, and 
ğœ–
Ïµ is the error term.

Explain Multiple Linear Regression.
It extends linear regression to include multiple independent variables:
ğ‘¦
=
ğ›½
0
+
ğ›½
1
ğ‘¥
1
+
ğ›½
2
ğ‘¥
2
+
â‹¯
+
ğ›½
ğ‘›
ğ‘¥
ğ‘›
+
ğœ–
y=Î² 
0
â€‹
 +Î² 
1
â€‹
 x 
1
â€‹
 +Î² 
2
â€‹
 x 
2
â€‹
 +â‹¯+Î² 
n
â€‹
 x 
n
â€‹
 +Ïµ

What is the difference between Linear and Logistic Regression?

Linear Regression: Used for predicting continuous outcomes.
Logistic Regression: Used for binary classification problems; predicts the probability that an instance belongs to a class using a sigmoid function.
What is Ridge and Lasso Regression?

Ridge Regression (L2 Regularization): Adds penalty proportional to the sum of squared coefficients.
CostÂ Function
=
RSS
+
ğœ†
âˆ‘
ğ›½
ğ‘—
2
CostÂ Function=RSS+Î»âˆ‘Î² 
j
2
â€‹
 
Lasso Regression (L1 Regularization): Adds penalty proportional to the sum of absolute coefficients.
CostÂ Function
=
RSS
+
ğœ†
âˆ‘
âˆ£
ğ›½
ğ‘—
âˆ£
CostÂ Function=RSS+Î»âˆ‘âˆ£Î² 
j
â€‹
 âˆ£
Explain the concept of Polynomial Regression.
It extends Linear Regression by adding polynomial terms to the model, enabling it to capture non-linear relationships:
ğ‘¦
=
ğ›½
0
+
ğ›½
1
ğ‘¥
+
ğ›½
2
ğ‘¥
2
+
â‹¯
+
ğ›½
ğ‘›
ğ‘¥
ğ‘›
y=Î² 
0
â€‹
 +Î² 
1
â€‹
 x+Î² 
2
â€‹
 x 
2
 +â‹¯+Î² 
n
â€‹
 x 
n
 

Model Evaluation and Metrics
Difference between Precision, Recall, and F1-score.
Precision: Ratio of true positives to total predicted positives.
Precision
=
ğ‘‡
ğ‘ƒ
ğ‘‡
ğ‘ƒ
+
ğ¹
ğ‘ƒ
Precision= 
TP+FP
TP
â€‹
 
Recall: Ratio of true positives to total actual positives.
Recall
=
ğ‘‡
ğ‘ƒ
ğ‘‡
ğ‘ƒ
+
ğ¹
ğ‘
Recall= 
TP+FN
TP
â€‹
 
F1-score: Harmonic mean of precision and recall.
F1
=
2
Ã—
Precision
Ã—
Recall
Precision
+
Recall
F1=2Ã— 
Precision+Recall
PrecisionÃ—Recall
â€‹
 
Explain Cross-Validation.
A technique to assess model performance by splitting the data into training and validation sets multiple times, commonly using k-fold cross-validation.

What is a Confusion Matrix?
A matrix used to evaluate classification models by showing true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN).

Explain Bias and Variance Tradeoff.

Bias: Error due to overly simplistic models.
Variance: Error due to model complexity causing sensitivity to data fluctuations.
A good model balances bias and variance.
What is the ROC Curve and AUC?

ROC Curve: Plots True Positive Rate (TPR) vs. False Positive Rate (FPR) at different thresholds.
AUC (Area Under Curve): Measures the model's ability to distinguish between classes.
Advanced Topics
What is Gradient Descent?
An optimization algorithm to minimize the loss function by updating the model's parameters iteratively in the direction of the negative gradient.

Explain Feature Selection and Feature Extraction.

Feature Selection: Selecting the most relevant features.
Feature Extraction: Transforming data into new features (e.g., PCA).
What is PCA (Principal Component Analysis)?
PCA reduces dimensionality by projecting data onto principal components that explain the most variance.

Difference between Bagging and Boosting.

Bagging: Reduces variance by training multiple models independently (e.g., Random Forest).
Boosting: Reduces bias by training models sequentially, correcting errors (e.g., AdaBoost, Gradient Boosting).
How do you handle missing data?

Remove missing values.
Impute missing values using mean, median, or mode.
Use advanced techniques like KNN imputation.
